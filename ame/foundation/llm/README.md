# LLM æ¨¡å— - åŒå±‚æ¶æ„

> **æ–°æ¶æ„å·²ä¸Šçº¿ï¼** æœ¬æ¨¡å—å·²é‡æ„ä¸ºåŒå±‚æ¶æ„ï¼Œæä¾›æ›´æ¸…æ™°çš„èŒè´£åˆ†ç¦»å’Œæ›´å¼ºçš„å¯æ‰©å±•æ€§ã€‚
> 
> - **æ¨èä½¿ç”¨**: `SessionPipe` (å¯¹è¯) / `DocumentPipe` (æ–‡æ¡£åˆ†æ)
> - **ä¼ ç»Ÿ API**: ä»ç„¶å¯ç”¨ï¼Œä½†å»ºè®®è¿ç§»åˆ°æ–°æ¶æ„
> - **è¿ç§»æŒ‡å—**: è§ [MIGRATION_GUIDE.md](./MIGRATION_GUIDE.md)

## å¿«é€Ÿå¼€å§‹ï¼ˆæ–°æ¶æ„ï¼‰

### 1. å¯¹è¯åœºæ™¯

```python
from ame.foundation.llm import AtomicOpenAICaller, SessionPipe, PipelineContext
from ame.foundation.llm.utils import ConversationMessage

# åˆ›å»ºè°ƒç”¨å™¨
caller = AtomicOpenAICaller(api_key="your-key", model="gpt-3.5-turbo")

# åˆ›å»ºä¼šè¯ç®¡é“
pipe = SessionPipe(
    caller=caller,
    cache_enabled=True,  # å¯ç”¨ç¼“å­˜
    keep_recent=5  # ä¿ç•™æœ€è¿‘5è½®å¯¹è¯
)

# å‡†å¤‡æ¶ˆæ¯
messages = [
    ConversationMessage(role="system", content="You are a helpful assistant."),
    ConversationMessage(role="user", content="Hello!"),
]

# è°ƒç”¨ç®¡é“
context = PipelineContext(messages=messages)
result = await pipe.process(context)

print(result.response.content)
print(f"ç¼“å­˜å‘½ä¸­: {result.cached}")
```

### 2. æ–‡æ¡£åˆ†æåœºæ™¯

```python
from ame.foundation.llm import AtomicOpenAICaller, DocumentPipe

caller = AtomicOpenAICaller(api_key="your-key")
pipe = DocumentPipe(caller=caller)

messages = [
    ConversationMessage(role="system", content="You are a document analyzer."),
    ConversationMessage(role="user", content=f"Analyze: {document_text}"),
]

context = PipelineContext(messages=messages)
result = await pipe.process(context)

print(result.response.content)
```

### 3. æµå¼è¾“å‡º

```python
context = PipelineContext(messages=messages, stream=True)
result = await pipe.process(context)

async for chunk in result.stream_iterator:
    print(chunk, end="", flush=True)
```

## æ¶æ„è®¾è®¡

### åŒå±‚æ¶æ„

```
åº”ç”¨å±‚ (Service/Controller)
    â”‚
    â†“
ç®¡é“å±‚ (Pipeline Layer) - åœºæ™¯åŒ–ç»„åˆ
    â”œâ”€ SessionPipe (å¯¹è¯ç®¡é“)
    â”‚   â”œâ”€ CacheStrategy (ç¼“å­˜)
    â”‚   â”œâ”€ SessionCompressStrategy (ä¿å®ˆå‹ç¼©)
    â”‚   â””â”€ RetryStrategy (é‡è¯•)
    â”‚
    â””â”€ DocumentPipe (æ–‡æ¡£ç®¡é“)
        â”œâ”€ DocumentCompressStrategy (æ¿€è¿›å‹ç¼©)
        â””â”€ RetryStrategy (é‡è¯•)
    â”‚
    â†“
åŸå­å±‚ (Atomic Layer) - åŸºç¡€èƒ½åŠ›
    â”œâ”€ Caller (è°ƒç”¨å™¨)
    â”‚   â”œâ”€ LLMCallerBase (æŠ½è±¡åŸºç±»)
    â”‚   â”œâ”€ OpenAICaller (ä¼˜åŒ–ç‰ˆï¼Œtiktoken)
    â”‚   â””â”€ StreamCaller (æµå¼å°è£…)
    â”‚
    â””â”€ Strategy (ç­–ç•¥)
        â”œâ”€ CacheStrategy (TTLCache)
        â”œâ”€ CompressStrategy (å‹ç¼©)
        â””â”€ RetryStrategy (é‡è¯•)
```

### æ ¸å¿ƒä¼˜åŠ¿

| ç»´åº¦ | æ–°æ¶æ„ | ä¼ ç»Ÿæ¶æ„ |
|------|--------|----------|
| **èŒè´£åˆ†ç¦»** | åŸå­èƒ½åŠ›ç‹¬ç«‹ã€ç®¡é“ç»„åˆ | èƒ½åŠ›è€¦åˆåœ¨ä¸€èµ· |
| **Tokenä¼°ç®—** | tiktokenç²¾ç¡®ä¼°ç®— | ç®€å•å…¬å¼ä¼°ç®— |
| **ç¼“å­˜æœºåˆ¶** | TTLCacheã€è‡ªåŠ¨è¿‡æœŸ | æ— ç¼“å­˜ |
| **ç­–ç•¥æ’ä»¶** | å¯æ’æ‹” | è€¦åˆåœ¨ç±»ä¸­ |
| **æ‰©å±•æ€§** | é«˜ | ä¸­ |

---

# ä¼ ç»Ÿ APIæ–‡æ¡£ï¼ˆå‘åå…¼å®¹ï¼‰

## æ ¸å¿ƒç†å¿µ

**æ–‡æ¡£åˆ†å—åˆ†æ = ç³»ç»Ÿé©±åŠ¨çš„å¤šè½®å¯¹è¯**

æ— è®ºæ˜¯ç”¨æˆ·ä¸»åŠ¨çš„å¤šè½®å¯¹è¯ï¼ˆSESSION æ¨¡å¼ï¼‰ï¼Œè¿˜æ˜¯ç³»ç»Ÿè‡ªåŠ¨çš„é•¿æ–‡æ¡£åˆ†æï¼ˆDOCUMENT æ¨¡å¼ï¼‰ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯åœ¨ç®¡ç†**å¯¹è¯å†å²**ï¼Œåªæ˜¯é©±åŠ¨æ–¹å¼å’Œå‹ç¼©ç­–ç•¥ä¸åŒã€‚

## æ¶æ„è®¾è®¡

```
ConversationHistory (å¯¹è¯å†å²ç®¡ç†)
        â”‚
        â”œâ”€â”€ CompressionStrategy (å‹ç¼©ç­–ç•¥)
        â”‚   â”œâ”€â”€ SessionCompressionStrategy (ä¼šè¯æ¨¡å¼ï¼šä¿å®ˆå‹ç¼©)
        â”‚   â”œâ”€â”€ DocumentCompressionStrategy (æ–‡æ¡£æ¨¡å¼ï¼šæ¿€è¿›å‹ç¼©)
        â”‚   â””â”€â”€ ChunkingCompressionStrategy (åˆ†å—æ¨¡å¼ï¼šè¶…é•¿æ–‡æœ¬)
        â”‚
        â””â”€â”€ ChunkedConversationManager (åˆ†å—å¯¹è¯ç®¡ç†)
            â””â”€â”€ åŸºäº ConversationHistory å®ç°é•¿æ–‡æ¡£æ¸è¿›å¼å¤„ç†
```

## ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: æ™®é€šå¤šè½®å¯¹è¯ï¼ˆSESSION æ¨¡å¼ï¼‰

```python
from foundation.llm import OpenAICaller, ContextMode

# åˆ›å»º LLM è°ƒç”¨å™¨
llm = OpenAICaller(
    api_key="your-api-key",
    max_context_tokens=4000
)

# åˆ›å»ºä¼šè¯å¯¹è¯
conversation = llm.create_conversation(
    system_prompt="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹",
    mode=ContextMode.SESSION  # ä¼šè¯æ¨¡å¼
)

# å¤šè½®å¯¹è¯
response1 = await llm.chat_with_history(
    conversation,
    "ä½ å¥½ï¼Œä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
)

response2 = await llm.chat_with_history(
    conversation,
    "æ¨èä¸€äº›æˆ·å¤–æ´»åŠ¨",
    important=True  # æ ‡è®°ä¸ºé‡è¦æ¶ˆæ¯ï¼ˆå‹ç¼©æ—¶ä¼˜å…ˆä¿ç•™ï¼‰
)

# å¯¹è¯ç»“æŸæ—¶ï¼Œå¯¼å‡ºå…³é”®ä¿¡æ¯åˆ°å›¾è°±
export_data = conversation.clear_and_export()
# âœ… å¯¼å‡º important=True çš„æ¶ˆæ¯ + æœ€è¿‘5è½®å¯¹è¯
```

### åœºæ™¯ 2: é•¿æ–‡æ¡£åˆ†æï¼ˆDOCUMENT æ¨¡å¼ + åˆ†å—ï¼‰

```python
from foundation.llm import (
    OpenAICaller,
    ContextMode,
    ChunkedConversationManager
)

# åˆ›å»º LLM è°ƒç”¨å™¨
llm = OpenAICaller(api_key="your-api-key")

# åˆ›å»ºæ–‡æ¡£åˆ†æå¯¹è¯
doc_conversation = llm.create_conversation(
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£åˆ†æåŠ©æ‰‹",
    mode=ContextMode.DOCUMENT  # æ–‡æ¡£æ¨¡å¼ï¼šé™é»˜å‹ç¼©
)

# åˆ›å»ºåˆ†å—ç®¡ç†å™¨
chunker = ChunkedConversationManager(
    conversation=doc_conversation,
    chunk_size=2000,      # æ¯å— 2000 tokens
    chunk_overlap=200     # é‡å  200 tokensï¼ˆä¿æŒè¿è´¯æ€§ï¼‰
)

# åˆ†å—é•¿æ–‡æ¡£
long_document = "..." # 10000+ tokens
chunks = chunker.split_into_chunks(long_document, llm.estimate_tokens)

# é€å—åˆ†æ
async def generate_fn(messages):
    return await llm.generate(messages)

def on_chunk_done(result):
    print(f"âœ… å®Œæˆç¬¬ {result.chunk_index + 1}/{result.total_chunks} å—åˆ†æ")
    print(f"è¿›åº¦: {chunker.get_progress()['progress_percentage']:.1f}%")

# å¤„ç†æ‰€æœ‰å—
results = await chunker.process_all_chunks(
    llm_generate_fn=generate_fn,
    on_chunk_complete=on_chunk_done
)

# ç”Ÿæˆæœ€ç»ˆæ€»ç»“
summary = await chunker.generate_final_summary(generate_fn)

# å¯¼å‡ºåˆ†ææŠ¥å‘Š
report = chunker.export_analysis_report()
# âœ… å¯¼å‡ºæ‰€æœ‰ LLM åˆ†æç»“æœ + æœ€ç»ˆæ€»ç»“
```

### åœºæ™¯ 3: æµå¼é•¿æ–‡æ¡£åˆ†æï¼ˆç”¨æˆ·å¯è§è¿›åº¦ï¼‰

```python
from foundation.llm import ChunkedConversationManager, ContextMode

async def stream_document_analysis(document: str):
    """æµå¼å±•ç¤ºæ–‡æ¡£åˆ†æè¿‡ç¨‹"""
    
    # åˆ›å»ºæ–‡æ¡£å¯¹è¯å’Œåˆ†å—ç®¡ç†å™¨
    llm = OpenAICaller(api_key="your-api-key")
    conversation = llm.create_conversation(mode=ContextMode.DOCUMENT)
    chunker = ChunkedConversationManager(conversation, chunk_size=2000)
    
    # åˆ†å—
    chunks = chunker.split_into_chunks(document, llm.estimate_tokens)
    
    # é€å—æµå¼åˆ†æ
    for i in range(len(chunks)):
        print(f"\nğŸ“„ æ­£åœ¨åˆ†æç¬¬ {i+1}/{len(chunks)} å—...")
        
        # æµå¼ç”Ÿæˆ
        full_response = ""
        async for chunk_text in llm.generate_stream(
            conversation.get_messages() + [
                {"role": "user", "content": chunks[i]}
            ]
        ):
            print(chunk_text, end="", flush=True)
            full_response += chunk_text
        
        # æ‰‹åŠ¨æ·»åŠ åˆ°å†å²
        conversation.add_message("user", chunks[i])
        conversation.add_message("assistant", full_response)
        
        # è‡ªåŠ¨å‹ç¼©ï¼ˆé™é»˜ï¼‰
        conversation.compress_if_needed(llm.estimate_tokens)
        
        print(f"\nâœ… ç¬¬ {i+1} å—åˆ†æå®Œæˆ")
    
    # æœ€ç»ˆæ€»ç»“
    print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ€»ç»“...")
    conversation.add_message("user", "è¯·æ€»ç»“ä¸Šè¿°æ‰€æœ‰åˆ†æ", important=True)
    
    summary = ""
    async for chunk in llm.generate_stream(conversation.get_messages()):
        print(chunk, end="", flush=True)
        summary += chunk
    
    conversation.add_message("assistant", summary, important=True)
    
    # å¯¼å‡º
    return conversation.export_important()
```

### åœºæ™¯ 4: è‡ªå®šä¹‰å‹ç¼©ç­–ç•¥

```python
from foundation.llm import (
    CompressionStrategy,
    ChunkingCompressionStrategy,
    ContextMode
)

# ä½¿ç”¨åˆ†å—å‹ç¼©ç­–ç•¥å¤„ç†è¶…é•¿æ¶ˆæ¯
chunking_strategy = ChunkingCompressionStrategy(chunk_size=3000)

conversation = llm.create_conversation(
    mode=ContextMode.DOCUMENT,
    compression_strategy=chunking_strategy  # è‡ªå®šä¹‰ç­–ç•¥
)

# æˆ–è€…ç»„åˆå¤šä¸ªç­–ç•¥
class HybridCompressionStrategy(CompressionStrategy):
    """æ··åˆç­–ç•¥ï¼šå…ˆå°è¯•åˆ†å—ï¼Œå†å°è¯•å‹ç¼©"""
    
    def __init__(self):
        self.chunking = ChunkingCompressionStrategy(chunk_size=2000)
        self.document = DocumentCompressionStrategy()
    
    def should_compress(self, messages, max_tokens, token_estimator):
        # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰è¶…é•¿æ¶ˆæ¯
        if self.chunking.should_compress(messages, max_tokens, token_estimator):
            return True
        # å†æ£€æŸ¥æ€»é‡æ˜¯å¦è¶…é™
        return self.document.should_compress(messages, max_tokens, token_estimator)
    
    def compress(self, messages, max_tokens, token_estimator):
        # å…ˆåˆ†å—
        if self.chunking.should_compress(messages, max_tokens, token_estimator):
            kept, removed = self.chunking.compress(messages, max_tokens, token_estimator)
            messages = kept
        
        # å†å‹ç¼©
        if self.document.should_compress(messages, max_tokens, token_estimator):
            return self.document.compress(messages, max_tokens, token_estimator)
        
        return messages, []
    
    def on_compression(self, removed_count, total_tokens, compressed_tokens):
        logger.info(f"æ··åˆå‹ç¼©å®Œæˆï¼šç§»é™¤ {removed_count} æ¡ï¼Œ{total_tokens} â†’ {compressed_tokens} tokens")
```

## å‹ç¼©ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | è§¦å‘é˜ˆå€¼ | ä¿ç•™ä¼˜å…ˆçº§ | æ—¥å¿—çº§åˆ« | é€‚ç”¨åœºæ™¯ |
|------|---------|----------|---------|---------|
| **SessionCompressionStrategy** | 95% | System > Important > Recent(5è½®) > Old | WARNING | ç”¨æˆ·å¯¹è¯ |
| **DocumentCompressionStrategy** | 80% | System > Latest User > Latest AI > Old AI | DEBUG | æ–‡æ¡£åˆ†æ |
| **ChunkingCompressionStrategy** | å•æ¡>70% | åˆ†å—ä¿ç•™æ‰€æœ‰å†…å®¹ | INFO | è¶…é•¿æ–‡æœ¬ |

## å¯¼å‡ºç­–ç•¥å¯¹æ¯”

### SESSION æ¨¡å¼å¯¼å‡º

```python
export_data = conversation.export_important()

# è¿”å›æ ¼å¼ï¼š
{
    "mode": "session",
    "total_conversations": 50,
    "important_count": 5,
    "export_content": [
        {
            "role": "user",
            "content": "...",
            "timestamp": "2024-01-01T10:00:00",
            "important": True
        },
        # ... æœ€è¿‘5è½®å¯¹è¯ ...
    ]
}
```

### DOCUMENT æ¨¡å¼å¯¼å‡º

```python
export_data = doc_conversation.export_important()

# è¿”å›æ ¼å¼ï¼š
{
    "mode": "document",
    "total_messages": 30,
    "analysis_count": 15,
    "export_content": {
        "llm_analysis": [
            {
                "content": "åˆ†æç»“æœ1...",
                "timestamp": "2024-01-01T10:00:00"
            },
            # ... æ‰€æœ‰ AI åˆ†æç»“æœ ...
        ],
        "important_inputs": [
            {
                "content": "é‡è¦æ–‡æ¡£ç‰‡æ®µ...",
                "timestamp": "2024-01-01T09:00:00"
            }
        ]
    }
}
```

## API å‚è€ƒ

### ConversationHistory

```python
conversation = ConversationHistory(
    max_context_tokens=4000,           # æœ€å¤§ä¸Šä¸‹æ–‡ token æ•°
    mode=ContextMode.SESSION,          # æ¨¡å¼ï¼šSESSION | DOCUMENT
    compression_strategy=None          # è‡ªå®šä¹‰å‹ç¼©ç­–ç•¥ï¼ˆå¯é€‰ï¼‰
)

# æ·»åŠ æ¶ˆæ¯
conversation.add_message("user", "Hello", important=True)

# å‹ç¼©ï¼ˆå¦‚æœéœ€è¦ï¼‰
conversation.compress_if_needed(token_estimator_fn)

# å¯¼å‡º
all_data = conversation.export_all()           # å®Œæ•´å†å²ï¼ˆå«å‹ç¼©è®°å½•ï¼‰
important_data = conversation.export_important()  # å…³é”®ä¿¡æ¯ï¼ˆæ ¹æ®æ¨¡å¼ï¼‰
graph_data = conversation.clear_and_export()   # æ¸…ç©ºå¹¶å¯¼å‡º

# ç»Ÿè®¡
stats = conversation.get_compression_stats()
```

### ChunkedConversationManager

```python
chunker = ChunkedConversationManager(
    conversation=doc_conversation,
    chunk_size=2000,
    chunk_overlap=200,
    chunking_mode=ChunkingMode.AUTO
)

# åˆ†å—
chunks = chunker.split_into_chunks(long_text, token_estimator)

# å¤„ç†
result = await chunker.process_chunk(0, llm_generate_fn)
all_results = await chunker.process_all_chunks(llm_generate_fn, on_complete_fn)

# æ€»ç»“
summary = await chunker.generate_final_summary(llm_generate_fn)

# å¯¼å‡º
report = chunker.export_analysis_report()
progress = chunker.get_progress()
```

## æœ€ä½³å®è·µ

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å¼

- **SESSION æ¨¡å¼**ï¼šç”¨æˆ·å¯¹è¯ã€éœ€è¦ä¿ç•™ä¸Šä¸‹æ–‡çš„åœºæ™¯
- **DOCUMENT æ¨¡å¼**ï¼šæ–‡æ¡£åˆ†æã€çŸ¥è¯†æå–ã€ä¸éœ€è¦ä¿ç•™å®Œæ•´å†å²

### 2. æ ‡è®°é‡è¦æ¶ˆæ¯

```python
# ä¼šè¯æ¨¡å¼ä¸­æ ‡è®°å…³é”®å¯¹è¯
await llm.chat_with_history(
    conversation,
    "è¿™ä¸ªå†³å®šå¾ˆé‡è¦ï¼Œè¯·è®°ä½",
    important=True  # å‹ç¼©æ—¶ä¼˜å…ˆä¿ç•™
)
```

### 3. ç›‘æ§å‹ç¼©ç»Ÿè®¡

```python
stats = conversation.get_compression_stats()
print(f"å·²å‹ç¼© {stats['total_compressions']} æ¬¡")
print(f"ç§»é™¤äº† {stats['total_messages_removed']} æ¡æ¶ˆæ¯")
```

### 4. åˆç†è®¾ç½® chunk_size

- **å°å—ï¼ˆ1000-1500ï¼‰**ï¼šé€‚åˆéœ€è¦ç»†ç²’åº¦åˆ†æçš„åœºæ™¯
- **ä¸­å—ï¼ˆ2000-3000ï¼‰**ï¼šæ¨èé»˜è®¤å€¼ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œæ•ˆç‡
- **å¤§å—ï¼ˆ4000+ï¼‰**ï¼šé€‚åˆæ¦‚è§ˆæ€§åˆ†æ

### 5. ä½¿ç”¨ overlap ä¿æŒè¿è´¯æ€§

```python
chunker = ChunkedConversationManager(
    conversation=doc_conversation,
    chunk_size=2000,
    chunk_overlap=200  # 10% é‡å ï¼Œé¿å…è¯­ä¹‰æ–­è£‚
)
```

## å¸¸è§é—®é¢˜

### Q: SESSION å’Œ DOCUMENT æ¨¡å¼å¯ä»¥äº’æ¢å—ï¼Ÿ

A: å¯ä»¥ï¼Œä½†ä¸æ¨èã€‚ä¸¤ç§æ¨¡å¼çš„å‹ç¼©ç­–ç•¥å’Œå¯¼å‡ºæ ¼å¼ä¸åŒï¼Œåˆ‡æ¢å¯èƒ½å¯¼è‡´è¡Œä¸ºä¸ä¸€è‡´ã€‚

### Q: å¦‚ä½•å¤„ç†æé•¿æ–‡æ¡£ï¼ˆ100k+ tokensï¼‰ï¼Ÿ

A: ä½¿ç”¨ `ChunkedConversationManager`ï¼Œå®ƒä¼šè‡ªåŠ¨åˆ†å—å¹¶é™é»˜å‹ç¼©å†å²åˆ†æç»“æœã€‚

### Q: å‹ç¼©åçš„æ¶ˆæ¯å»å“ªäº†ï¼Ÿ

A: è¢«ç§»é™¤çš„æ¶ˆæ¯ä¼šå­˜å‚¨åœ¨ `conversation._archived_messages` ä¸­ï¼Œå¯ä»¥é€šè¿‡ `export_all()` å¯¼å‡ºã€‚

### Q: å¦‚ä½•ç¦ç”¨è‡ªåŠ¨å‹ç¼©ï¼Ÿ

A: è®¾ç½® `max_context_tokens=None`ï¼š

```python
conversation = llm.create_conversation(
    mode=ContextMode.SESSION,
    max_context_tokens=None  # ä¸é™åˆ¶ï¼Œä¸å‹ç¼©
)
```

## æ€§èƒ½å»ºè®®

1. **å¯ç”¨ç¼“å­˜**ï¼ˆä»…é™éåˆ›é€ æ€§ä»»åŠ¡ï¼‰ï¼š
   ```python
   llm = OpenAICaller(cache_enabled=True)
   ```

2. **åˆç†è®¾ç½® max_context_tokens**ï¼š
   - GPT-3.5-turbo: 4000
   - GPT-4: 8000
   - GPT-4-32k: 32000

3. **ä½¿ç”¨æµå¼è¾“å‡ºæå‡ä½“éªŒ**ï¼š
   ```python
   async for chunk in llm.chat_stream_with_history(conversation, message):
       print(chunk, end="", flush=True)
   ```

## æ›´æ–°æ—¥å¿—

### v1.0.0 (2024-01)
- âœ… ç»Ÿä¸€ä¼šè¯æ¨¡å¼å’Œæ–‡æ¡£æ¨¡å¼çš„æ¶æ„
- âœ… å¼•å…¥ç­–ç•¥æ¨¡å¼å®ç°å¯æ‰©å±•çš„å‹ç¼©ç­–ç•¥
- âœ… å®ç° ChunkedConversationManager å¤„ç†é•¿æ–‡æ¡£
- âœ… æ”¯æŒè‡ªå®šä¹‰å‹ç¼©ç­–ç•¥
- âœ… å®Œå–„çš„å¯¼å‡ºæœºåˆ¶å’Œç»Ÿè®¡ä¿¡æ¯
