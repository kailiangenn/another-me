"""
OpenAICaller è„šæœ¬åŒ–æµ‹è¯•

éœ€è¦çœŸå®çš„OpenAI API Keyæ¥è¿è¡Œæµ‹è¯•
"""

import asyncio
import sys
import os
from pathlib import Path

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../..'))

from ame import OpenAICaller, LLMResponse


def print_separator(title=""):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print(f"\n{'='*80}")
        print(f"  {title}")
        print(f"{'='*80}")
    else:
        print("-" * 80)


async def test_token_estimation(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•Tokenä¼°ç®—"""
    print_separator("æµ‹è¯•Tokenä¼°ç®—")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    
    # æµ‹è¯•è‹±æ–‡
    text_en = "Hello world, this is a test."
    tokens_en = caller.estimate_tokens(text_en)
    print(f"\nè‹±æ–‡æ–‡æœ¬: {text_en}")
    print(f"ä¼°ç®—Tokenæ•°: {tokens_en}")
    
    # æµ‹è¯•ä¸­æ–‡
    text_cn = "ä½ å¥½ä¸–ç•Œï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚"
    tokens_cn = caller.estimate_tokens(text_cn)
    print(f"\nä¸­æ–‡æ–‡æœ¬: {text_cn}")
    print(f"ä¼°ç®—Tokenæ•°: {tokens_cn}")
    
    # æµ‹è¯•æ··åˆ
    text_mix = "Hello ä½ å¥½ world ä¸–ç•Œ"
    tokens_mix = caller.estimate_tokens(text_mix)
    print(f"\næ··åˆæ–‡æœ¬: {text_mix}")
    print(f"ä¼°ç®—Tokenæ•°: {tokens_mix}")
    
    # æµ‹è¯•æ¶ˆæ¯åˆ—è¡¨
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"},
        {"role": "assistant", "content": "Hi there!"}
    ]
    total_tokens = caller.estimate_messages_tokens(messages)
    print(f"\næ¶ˆæ¯åˆ—è¡¨:")
    for msg in messages:
        print(f"  [{msg['role']}] {msg['content']}")
    print(f"æ€»Tokenä¼°ç®—: {total_tokens}")
    
    print("\nâœ… Tokenä¼°ç®—æµ‹è¯•å®Œæˆ")


async def test_basic_generate(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•åŸºæœ¬ç”Ÿæˆ"""
    print_separator("æµ‹è¯•åŸºæœ¬ç”Ÿæˆï¼ˆCompleteæ¨¡å¼ï¼‰")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    
    print(f"\né…ç½®çŠ¶æ€: {'å·²é…ç½®' if caller.is_configured() else 'æœªé…ç½®'}")
    print(f"ä½¿ç”¨æ¨¡å‹: {caller.model}")
    
    messages = [
        {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»Pythonç¼–ç¨‹è¯­è¨€"}
    ]
    
    print(f"\nå‘é€æ¶ˆæ¯: {messages[0]['content']}")
    print("\nç­‰å¾…å“åº”...")
    
    response = await caller.generate(
        messages=messages,
        temperature=0.7,
        max_tokens=100
    )
    
    print(f"\nğŸ“ å“åº”å†…å®¹:")
    print(f"   {response.content}")
    print(f"\nğŸ“Š ä½¿ç”¨ç»Ÿè®¡:")
    print(f"   æ¨¡å‹: {response.model}")
    print(f"   æç¤ºToken: {response.prompt_tokens}")
    print(f"   ç”ŸæˆToken: {response.completion_tokens}")
    print(f"   æ€»Token: {response.total_tokens}")
    print(f"   å®ŒæˆåŸå› : {response.finish_reason}")
    
    print("\nâœ… åŸºæœ¬ç”Ÿæˆæµ‹è¯•å®Œæˆ")


async def test_stream_generate(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•æµå¼ç”Ÿæˆ"""
    print_separator("æµ‹è¯•æµå¼ç”Ÿæˆï¼ˆStreamæ¨¡å¼ï¼‰")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    
    messages = [
        {"role": "user", "content": "ç”¨ä¸‰å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹"}
    ]
    
    print(f"\nå‘é€æ¶ˆæ¯: {messages[0]['content']}")
    print("\nğŸ“ æµå¼å“åº”:")
    print("   ", end="", flush=True)
    
    full_response = ""
    async for chunk in caller.generate_stream(
        messages=messages,
        temperature=0.7,
        max_tokens=200
    ):
        print(chunk, end="", flush=True)
        full_response += chunk
    
    print("\n")
    print(f"\nå®Œæ•´å“åº”é•¿åº¦: {len(full_response)} å­—ç¬¦")
    print(f"ä¼°ç®—Tokenæ•°: {caller.estimate_tokens(full_response)}")
    
    print("\nâœ… æµå¼ç”Ÿæˆæµ‹è¯•å®Œæˆ")


async def test_multi_turn_conversation(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•å¤šè½®å¯¹è¯"""
    print_separator("æµ‹è¯•å¤šè½®å¯¹è¯")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": "ä½ å¥½ï¼Œæˆ‘æƒ³å­¦ä¹ ç¼–ç¨‹"},
    ]
    
    print("\nğŸ’¬ å¯¹è¯å¼€å§‹")
    
    # ç¬¬ä¸€è½®
    print(f"\nğŸ‘¤ ç”¨æˆ·: {messages[-1]['content']}")
    response1 = await caller.generate(messages, temperature=0.7, max_tokens=100)
    print(f"ğŸ¤– åŠ©æ‰‹: {response1.content}")
    
    messages.append({"role": "assistant", "content": response1.content})
    messages.append({"role": "user", "content": "æ¨èä¸€ä¸ªé€‚åˆåˆå­¦è€…çš„è¯­è¨€"})
    
    # ç¬¬äºŒè½®
    print(f"\nğŸ‘¤ ç”¨æˆ·: {messages[-1]['content']}")
    response2 = await caller.generate(messages, temperature=0.7, max_tokens=100)
    print(f"ğŸ¤– åŠ©æ‰‹: {response2.content}")
    
    print(f"\nğŸ“Š å¯¹è¯ç»Ÿè®¡:")
    print(f"   æ€»è½®æ¬¡: 2")
    print(f"   æ€»æ¶ˆæ¯æ•°: {len(messages) + 1}")
    total_tokens = caller.estimate_messages_tokens(messages) + response2.completion_tokens
    print(f"   ä¼°ç®—æ€»Token: {total_tokens}")
    
    print("\nâœ… å¤šè½®å¯¹è¯æµ‹è¯•å®Œæˆ")


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print_separator("OpenAICaller è„šæœ¬åŒ–æµ‹è¯•")
    
    # è·å–API Key
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("\nè¯·è¾“å…¥OpenAI API Key:")
        api_key = input("> ").strip()
    
    if not api_key:
        print("\nâŒ é”™è¯¯: æœªæä¾›API Key")
        return
    
    # è·å–Base URLï¼ˆå¯é€‰ï¼‰
    base_url = os.environ.get("OPENAI_BASE_URL")
    if not base_url:
        print("\nè¯·è¾“å…¥Base URL (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤):")
        base_url_input = input("> ").strip()
        base_url = base_url_input if base_url_input else None
    
    # è·å–æ¨¡å‹åç§°
    model_name = os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo")
    print(f"\nè¯·è¾“å…¥æ¨¡å‹åç§° (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤: {model_name}):")
    model_input = input("> ").strip()
    if model_input:
        model_name = model_input
    
    print(f"\nâœ… é…ç½®ä¿¡æ¯:")
    print(f"   API Key: {api_key[:8]}...")
    print(f"   Base URL: {base_url or 'é»˜è®¤'}")
    print(f"   Model: {model_name}")
    
    try:
        # æ‰§è¡Œæµ‹è¯•
        await test_token_estimation(api_key, base_url, model_name)
        await test_basic_generate(api_key, base_url, model_name)
        await test_stream_generate(api_key, base_url, model_name)
        await test_multi_turn_conversation(api_key, base_url, model_name)
        
        print_separator("æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
