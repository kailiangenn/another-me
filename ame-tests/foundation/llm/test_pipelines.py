"""
ç®¡é“è„šæœ¬åŒ–æµ‹è¯•

æµ‹è¯•SessionPipeå’ŒDocumentPipeçš„çœŸå®åŠŸèƒ½
"""

import asyncio
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../../..'))

from ame import OpenAICaller, SessionPipe, DocumentPipe, PipelineContext


def print_separator(title=""):
    """æ‰“å°åˆ†éš”çº¿"""
    if title:
        print(f"\n{'='*80}")
        print(f"  {title}")
        print(f"{'='*80}")
    else:
        print("-" * 80)


async def test_session_basic(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•SessionPipeåŸºæœ¬åŠŸèƒ½"""
    print_separator("æµ‹è¯•SessionPipeåŸºæœ¬åŠŸèƒ½")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    pipe = SessionPipe(caller, cache_enabled=True)
    
    messages = [
        {"role": "user", "content": "ç”¨ä¸€å¥è¯ä»‹ç»æœºå™¨å­¦ä¹ "}
    ]
    
    print(f"\nå‘é€æ¶ˆæ¯: {messages[0]['content']}")
    
    context = PipelineContext(messages=messages, max_tokens=4000, temperature=0.7)
    result = await pipe.process(context)
    
    print(f"\nğŸ“ å“åº”å†…å®¹:")
    print(f"   {result.response.content}")
    print(f"\nğŸ“Š ç®¡é“ä¿¡æ¯:")
    print(f"   æ¨¡å¼: {result.metadata.get('mode', 'unknown')}")
    print(f"   ç¼“å­˜: {'å‘½ä¸­' if result.cached else 'æœªå‘½ä¸­'}")
    print(f"   å‹ç¼©: {'æ˜¯' if result.compressed else 'å¦'}")
    print(f"   æ€»Token: {result.response.total_tokens}")
    
    print("\nâœ… SessionPipeåŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_session_cache(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•SessionPipeç¼“å­˜åŠŸèƒ½"""
    print_separator("æµ‹è¯•SessionPipeç¼“å­˜åŠŸèƒ½")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    pipe = SessionPipe(caller, cache_enabled=True)
    
    messages = [
        {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"}
    ]
    
    context = PipelineContext(messages=messages, max_tokens=4000, temperature=0.7)
    
    # ç¬¬ä¸€æ¬¡è°ƒç”¨
    print("\nç¬¬ä¸€æ¬¡è°ƒç”¨...")
    result1 = await pipe.process(context)
    print(f"å“åº”: {result1.response.content[:50]}...")
    print(f"ç¼“å­˜çŠ¶æ€: {'å‘½ä¸­' if result1.cached else 'æœªå‘½ä¸­'}")
    
    # ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
    print("\nç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆç›¸åŒè¾“å…¥ï¼‰...")
    result2 = await pipe.process(context)
    print(f"å“åº”: {result2.response.content[:50]}...")
    print(f"ç¼“å­˜çŠ¶æ€: {'å‘½ä¸­' if result2.cached else 'æœªå‘½ä¸­'}")
    
    # ç¼“å­˜ç»Ÿè®¡
    stats = pipe.get_cache_stats()
    print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
    print(f"   å¯ç”¨: {stats['enabled']}")
    print(f"   å½“å‰å¤§å°: {stats['size']}")
    print(f"   æœ€å¤§å®¹é‡: {stats['max_size']}")
    
    print("\nâœ… SessionPipeç¼“å­˜åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_session_compression(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•SessionPipeå‹ç¼©åŠŸèƒ½"""
    print_separator("æµ‹è¯•SessionPipeå‹ç¼©åŠŸèƒ½")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    pipe = SessionPipe(
        caller,
        compress_threshold=0.5,  # é™ä½é˜ˆå€¼ä»¥ä¾¿è§¦å‘
        keep_recent=1
    )
    
    # åˆ›å»ºé•¿å¯¹è¯å†å²
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹"}
    ]
    
    for i in range(5):
        messages.append({
            "role": "user",
            "content": f"è¿™æ˜¯ç¬¬{i+1}ä¸ªé—®é¢˜ï¼šè¯·è¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½çš„ç¬¬{i+1}ä¸ªåº”ç”¨åœºæ™¯ã€‚" * 10
        })
        messages.append({
            "role": "assistant",
            "content": f"å…³äºç¬¬{i+1}ä¸ªåº”ç”¨åœºæ™¯çš„è¯¦ç»†è¯´æ˜..." * 20
        })
    
    messages.append({"role": "user", "content": "æ€»ç»“ä¸€ä¸‹å‰é¢çš„å†…å®¹"})
    
    print(f"\næ€»æ¶ˆæ¯æ•°: {len(messages)}")
    print(f"ä¼°ç®—Tokenæ•°: {caller.estimate_messages_tokens(messages)}")
    
    context = PipelineContext(messages=messages, max_tokens=1000, temperature=0.7)
    result = await pipe.process(context)
    
    print(f"\nğŸ“ å“åº”: {result.response.content[:100]}...")
    print(f"\nğŸ“Š å‹ç¼©ä¿¡æ¯:")
    print(f"   æ˜¯å¦å‹ç¼©: {result.compressed}")
    if result.compressed and result.compression_info:
        print(f"   å‹ç¼©å‰Token: {result.compression_info.get('tokens_before', 0)}")
        print(f"   å‹ç¼©åToken: {result.compression_info.get('tokens_after', 0)}")
        print(f"   å‹ç¼©æ¯”: {result.compression_info.get('compression_ratio', 0):.2%}")
        print(f"   ç§»é™¤æ¶ˆæ¯æ•°: {result.compression_info.get('removed_count', 0)}")
    
    print("\nâœ… SessionPipeå‹ç¼©åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_document_basic(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•DocumentPipeåŸºæœ¬åŠŸèƒ½"""
    print_separator("æµ‹è¯•DocumentPipeåŸºæœ¬åŠŸèƒ½")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    pipe = DocumentPipe(caller)
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æåŠ©æ‰‹"},
        {"role": "user", "content": "åˆ†æè¿™ç¯‡æ–‡æ¡£çš„ä¸»é¢˜ï¼šäººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œï¼Œæ·±åº¦å­¦ä¹ æ˜¯å…¶æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚"}
    ]
    
    print(f"\nå‘é€æ¶ˆæ¯: {messages[1]['content']}")
    
    context = PipelineContext(messages=messages, max_tokens=4000, temperature=0.7)
    result = await pipe.process(context)
    
    print(f"\nğŸ“ åˆ†æç»“æœ:")
    print(f"   {result.response.content}")
    print(f"\nğŸ“Š ç®¡é“ä¿¡æ¯:")
    print(f"   æ¨¡å¼: {result.metadata.get('mode', 'unknown')}")
    print(f"   ç®¡é“ç±»å‹: {result.metadata.get('pipeline', 'unknown')}")
    print(f"   ç¼“å­˜: {'å‘½ä¸­' if result.cached else 'æœªå‘½ä¸­'}")
    print(f"   å‹ç¼©: {'æ˜¯' if result.compressed else 'å¦'}")
    
    print("\nâœ… DocumentPipeåŸºæœ¬åŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def test_document_compression(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•DocumentPipeæ¿€è¿›å‹ç¼©"""
    print_separator("æµ‹è¯•DocumentPipeæ¿€è¿›å‹ç¼©")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    pipe = DocumentPipe(caller, compress_threshold=0.5)
    
    # åˆ›å»ºå¤šè½®åˆ†æå†å²
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æåŠ©æ‰‹"}
    ]
    
    for i in range(5):
        messages.append({
            "role": "user",
            "content": f"åˆ†ææ–‡æ¡£ç‰‡æ®µ{i+1}ï¼š" + "è¿™æ˜¯ä¸€æ®µå¾ˆé•¿çš„æ–‡æ¡£å†…å®¹ï¼ŒåŒ…å«äº†å¤§é‡çš„æŠ€æœ¯ç»†èŠ‚å’Œè¯´æ˜ã€‚" * 30
        })
        messages.append({
            "role": "assistant",
            "content": f"å¯¹ç‰‡æ®µ{i+1}çš„åˆ†æç»“æœ..." * 40
        })
    
    messages.append({"role": "user", "content": "è¯·æ€»ç»“æ‰€æœ‰ç‰‡æ®µ"})
    
    print(f"\næ€»æ¶ˆæ¯æ•°: {len(messages)}")
    print(f"ä¼°ç®—Tokenæ•°: {caller.estimate_messages_tokens(messages)}")
    
    context = PipelineContext(messages=messages, max_tokens=1000, temperature=0.7)
    result = await pipe.process(context)
    
    print(f"\nğŸ“ å“åº”: {result.response.content[:100]}...")
    print(f"\nğŸ“Š å‹ç¼©ä¿¡æ¯:")
    print(f"   æ˜¯å¦å‹ç¼©: {result.compressed}")
    if result.compressed and result.compression_info:
        print(f"   å‹ç¼©å‰Token: {result.compression_info.get('tokens_before', 0)}")
        print(f"   å‹ç¼©åToken: {result.compression_info.get('tokens_after', 0)}")
        print(f"   å‹ç¼©æ¯”: {result.compression_info.get('compression_ratio', 0):.2%}")
        print(f"   ç§»é™¤æ¶ˆæ¯æ•°: {result.compression_info.get('removed_count', 0)}")
    
    print("\nâœ… DocumentPipeæ¿€è¿›å‹ç¼©æµ‹è¯•å®Œæˆ")


async def test_session_export(api_key: str, base_url: str = None, model_name: str = "gpt-3.5-turbo"):
    """æµ‹è¯•ä¼šè¯å¯¼å‡ºåŠŸèƒ½"""
    print_separator("æµ‹è¯•ä¼šè¯å¯¼å‡ºåŠŸèƒ½")
    
    caller = OpenAICaller(api_key=api_key, model=model_name, base_url=base_url)
    pipe = SessionPipe(caller)
    
    # è¿›è¡Œå‡ è½®å¯¹è¯
    messages = [
        {"role": "user", "content": "ä½ å¥½"}
    ]
    
    context = PipelineContext(messages=messages)
    result1 = await pipe.process(context)
    
    messages.append({"role": "assistant", "content": result1.response.content})
    messages.append({"role": "user", "content": "ä»‹ç»ä¸€ä¸‹AI"})
    
    context = PipelineContext(messages=messages)
    result2 = await pipe.process(context)
    
    # å¯¼å‡ºä¼šè¯
    session_data = pipe.export_session()
    
    print(f"\nğŸ“¦ å¯¼å‡ºæ•°æ®:")
    print(f"   ç±»å‹: {session_data.get('type')}")
    print(f"   æ¶ˆæ¯æ•°: {session_data['history']['total_messages']}")
    print(f"   å‹ç¼©äº‹ä»¶: {len(session_data['history']['compression_events'])}")
    print(f"   å¯¼å‡ºæ—¶é—´: {session_data.get('exported_at')}")
    
    print("\nâœ… ä¼šè¯å¯¼å‡ºåŠŸèƒ½æµ‹è¯•å®Œæˆ")


async def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print_separator("ç®¡é“è„šæœ¬åŒ–æµ‹è¯•")
    
    # è·å–API Key
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        print("\nè¯·è¾“å…¥OpenAI API Key:")
        api_key = input("> ").strip()
    
    if not api_key:
        print("\nâŒ é”™è¯¯: æœªæä¾›API Key")
        return
    
    # è·å–Base URLï¼ˆå¯é€‰ï¼‰
    base_url = os.environ.get("OPENAI_BASE_URL")
    if not base_url:
        print("\nè¯·è¾“å…¥Base URL (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤):")
        base_url_input = input("> ").strip()
        base_url = base_url_input if base_url_input else None
    
    # è·å–æ¨¡å‹åç§°
    model_name = os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo")
    print(f"\nè¯·è¾“å…¥æ¨¡å‹åç§° (ç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤: {model_name}):")
    model_input = input("> ").strip()
    if model_input:
        model_name = model_input
    
    print(f"\nâœ… é…ç½®ä¿¡æ¯:")
    print(f"   API Key: {api_key[:8]}...")
    print(f"   Base URL: {base_url or 'é»˜è®¤'}")
    print(f"   Model: {model_name}")
    
    try:
        # æ‰§è¡Œæµ‹è¯•
        await test_session_basic(api_key, base_url, model_name)
        await test_session_cache(api_key, base_url, model_name)
        await test_session_compression(api_key, base_url, model_name)
        await test_document_basic(api_key, base_url, model_name)
        await test_document_compression(api_key, base_url, model_name)
        await test_session_export(api_key, base_url, model_name)
        
        print_separator("æ‰€æœ‰æµ‹è¯•å®Œæˆ")
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
